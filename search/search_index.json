{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"blog/_index/","text":"","title":" index"},{"location":"blog/articles/_index/","text":"","title":" index"},{"location":"community/adopters/","text":"Adopters of SAME \u00b6","title":"Adopters of SAME"},{"location":"community/adopters/#adopters-of-same","text":"","title":"Adopters of SAME"},{"location":"community/presentations/","text":"SAME \u00b6","title":"SAME"},{"location":"community/presentations/#same","text":"","title":"SAME"},{"location":"developer/debug/","text":"SAME Debugging Guide \u00b6","title":"SAME Debugging Guide"},{"location":"developer/debug/#same-debugging-guide","text":"","title":"SAME Debugging Guide"},{"location":"developer/developer/","text":"","title":"Developer"},{"location":"getting-started/_index/","text":"","title":"How to Get Started with SAME"},{"location":"getting-started/adding-steps/","text":"In order to add steps, we need a comment in the notebook itself. This can be done by adding a tag in the notebook, or a comment in the python file. Allowing for Adding Specific Tags \u00b6 First, go to the notebook settings and view \"Cell Metadata\": Add a Tag that Specifies a Step \u00b6 Second, go to the cell you want to split steps with, and add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.) Execute the step \u00b6 Now, when you execute same program run the cells will automatically be grouped together into steps, and executed serially. No additional work is necessary for the end user. We inject two additional steps for each graph, a \"run info\" step (which is necessary because we only know things like Run ID when the step is executed), and the \"context\" step (which adds a default, well-formed context dictionary to the graph).","title":"Adding Steps"},{"location":"getting-started/adding-steps/#allowing-for-adding-specific-tags","text":"First, go to the notebook settings and view \"Cell Metadata\":","title":"Allowing for Adding Specific Tags"},{"location":"getting-started/adding-steps/#add-a-tag-that-specifies-a-step","text":"Second, go to the cell you want to split steps with, and add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.)","title":"Add a Tag that Specifies a Step"},{"location":"getting-started/adding-steps/#execute-the-step","text":"Now, when you execute same program run the cells will automatically be grouped together into steps, and executed serially. No additional work is necessary for the end user. We inject two additional steps for each graph, a \"run info\" step (which is necessary because we only know things like Run ID when the step is executed), and the \"context\" step (which adds a default, well-formed context dictionary to the graph).","title":"Execute the step"},{"location":"getting-started/changing-environment/","text":"Often times, a specific step will require a particular base image to execute inside of. This may be because the image was pre-built with particular dependencies (packages, libraries, drivers, etc) which Jupyter does not need to execute, but may be necessary on the back end. This feature allows to specify the base image required to execute a given step. Editing the Notebook to Allow Tags \u00b6 First, go to the notebook settings and view \"Cell Metadata\": Adding an Environment Specifier \u00b6 You can add alternate base images with the following tag structure: environment = environment_name For example: environment = default or: environment = private-training-env Update SAME File \u00b6 Then in your SAME file, you'll add a section that maps to the specific image you'll need.","title":"Changing Environment"},{"location":"getting-started/changing-environment/#editing-the-notebook-to-allow-tags","text":"First, go to the notebook settings and view \"Cell Metadata\":","title":"Editing the Notebook to Allow Tags"},{"location":"getting-started/changing-environment/#adding-an-environment-specifier","text":"You can add alternate base images with the following tag structure: environment = environment_name For example: environment = default or: environment = private-training-env","title":"Adding an Environment Specifier"},{"location":"getting-started/changing-environment/#update-same-file","text":"Then in your SAME file, you'll add a section that maps to the specific image you'll need.","title":"Update SAME File"},{"location":"getting-started/dev-build/","text":"If you are using the same-mono-private repo, it does not currently produce a binary that can be installed from https://get.sameproject.org/. You will need to clone the repo and run the CLI from the main branch: Prerequisites \u00b6 Python 3.8 . Note that 3.9 is not currently supported due to Azure Machine Learning dependencies. Poetry 1.1.7 or higher. Azure CLI 2.27 or higher. Azure Functions Work Tools v3.x or higher (Optional). Clone the repo to your local machine and initialize the submodules: git clone https://github.com/SAME-Project/same-mono-private.git cd same-mono-private git submodule update --init --recursive Download and install Poetry, which is used to manage dependencies and virtual environments for the SAME project. You will need to install the project's Python dependencies using Poetry as well after installing it: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python3 - poetry install To install AML dependencies, now optional, use poetry install --extras azureml Using the devcontainer with Visual Studio Code \u00b6 If you are using Visual Studio Code (VSCode) to work with the same-mono-private repo, VSCode supports development in a containerized environment through its Remote - Container extension , so you don't need to manually install all of the tools and frameworks yourself. Prerequisites \u00b6 Docker For Windows users, we recommend enabling WSL2 back-end integration with Docker . Visual Studio Code Visual Studio Code Remote - Containers extension Opening same-mono-private in a devcontainer \u00b6 After you have cloned the dapr repo locally, open the dapr folder in VSCode. For example: git clone https://github.com/SAME-Project/same-mono-private.git cd same-mono-private git submodule update --init --recursive code . VSCode will detect the presence of a dev container definition in the repo and will prompt you to reopen the project in a container. Alternatively, you can open the command palette and use the Remote-Containers: Reopen in Container command. Once the container is loaded, open an integrated terminal in VSCode and you're ready to use the repo. You will still need to install the project Python dependencies using the preinstalled Poetry tool: poetry install To install AML dependencies, now optional, use poetry install --extras azureml Using the repo \u00b6 Use of the SAME python project assumes executing in a virtual environment managed by Poetry. Before running any commands, the virtual environment should be started: poetry shell NOTE: From this point forward, all functions require executing inside that virtual environment. If you see an error like zsh: command not found , it could be because you're not executing inside the venv. You can check this by executing: which python3 This should result in a response like: .../pypoetry/virtualenvs/same-mono-private-88mixeKa-py3.8/bin/python3 . If it reports something like /usr/bin/python or /usr/local/bin/python , you are using the system python, and things will not work as expected. How to execute against a notebook from source code \u00b6 From the root of project, execute: python3 cli/same/main.py <cli-arguments> TODO: Enable building the CLI into a redistributable binary via something like PyOxidiser in same-mono-private. When we get to binary builds of the CLI that can be run locally, you can execute the local build with: bin/same <cli-arguments> After we start publishing builds, you can install and execute with the following: curl -L0 https://get.sameproject.org/ | bash - same <cli-arguments> How to run the tests in the repo \u00b6 Setup kubeconfig to Azure Kubernetes Service configured for Kubeflow To run the CLI pytests, you will need a Kubernetes cluster configured with Kubeflow. We already have a Kubernetes cluster set up in the SAME-sample-vm_group resource group. To use that cluster as a member of the SAME Dev subscription, import the cluster credentials to your local kubeconfig as the current context: az login az account set -s \"SAME Dev\" az aks get-credentials --name AKSMLProductionCluster --resource-group SAME-sample-vm_group NOTE: You will also need to be logged into Azure with the SAME Dev subscription every time to run the tests. The Azure login will also allow the Durable Functions backend tests to run. Setup the environment variables for Azure Machine Learning (AML) To run the AML tests, the local environment variables for AML must be populated. You can set them by providing a file named .env.sh at the root of the directory containing the following export statements: # Set using the following instructions: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#configure-a-service-principal export AML_SP_APP_ID = <clientId from ` az ad sp create-for-rbac ` > export AML_SP_TENANT_ID = <tenantId from ` az ad sp create-for-rbac ` > export AML_SP_PASSWORD_VALUE = <clientSecret from ` az ad sp create-for-rbac ` > # Set using the following instructions: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli#set-up export WORKSPACE_SUBSCRIPTION_ID = <id from ` az account show ` when creating the workspace> export WORKSPACE_RESOURCE_GROUP = <resource-group value passed to ` az ml workspace create ` > export WORKSPACE_NAME = <workgroup-name value passed to ` az ml workspace create ` > # Compute name instance that can be setup using: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=azure-cli#create export AML_COMPUTE_NAME = <name passed to ` az ml compute create ` > For example, using the pre-configured SAME-test-aml-rg available to the SAME Dev subscription: #!/usr/bin/env bash export AML_SP_APP_ID = \"2dd71611-83d6-4950-b4b2-ccfb6efe6528\" export AML_SP_TENANT_ID = \"72f988bf-86f1-41af-91ab-2d7cd011db47\" export AML_SP_PASSWORD_VALUE = <copy from same-aml-test-sp ` clientSecret ` in the same-infra-keyvault> export WORKSPACE_SUBSCRIPTION_ID = \"1367ca4d-9e6c-4c41-937f-c657878ee8d5\" export WORKSPACE_RESOURCE_GROUP = \"SAME-test-aml-rg\" export WORKSPACE_NAME = \"SAME-test-aml-workspace\" export AML_COMPUTE_NAME = \"SAME-test-aml-compute\" Run the tests To run all the tests against the CLI and SDK: pytest To run a subset of tests for a single file: pytest test/cli/test_<file>.py -k \"test_<name>\" How to setup private test environments \u00b6 Local Kubeflow cluster on Minikube in devcontainer \u00b6 The devcontainer image for same-mono-private comes with minikube preinstalled, so you can set up a local Kubeflow cluster to run the CLI pytests against if you wish: Start a minikube cluster in the devcontainer: Note: Kubeflow currently defines its Custom Resource Definitions (CRD) under apiextensions.k8s.io/v1beta which is deprecated in Kubernetes v1.22, so minikube must start the cluster with a version <1.22. See kubeflow/kfctl issue #500 . minikube start --kubernetes-version = v1.21.5 Starting minikube will also change the default kubeconfig context to the minikube cluster. You can check this with: kubectl config get-contexts Deploy Kubeflow to the minikube cluster: export PIPELINE_VERSION = 1 .7.0 kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= $PIPELINE_VERSION \" kubectl wait --for condition = established --timeout = 60s crd/applications.app.k8s.io kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= $PIPELINE_VERSION \" Kubeflow cluster on Azure Kubernetes Services (AKS) \u00b6 From any Azure subscription where you are at least a Contributor, you can create and provision a new AKS cluster with Kubeflow: Create a new AKS cluster either using the Azure CLI or Azure Portal . The linked instructions will also update your kubeconfig to use the new cluster as the context when you run az aks get-credentials , but you can also manually do so with: kubectl config set-context <context name> Deploy Kubeflow to the cluster. Note: The document references a non-existent v1.3.0 release, you can simply use the v1.2.0 release instead. See kubeflow/kfctl issue #495 . Azure Machine Learning (AML) workspace and compute \u00b6 Create a new Service Principal for running tests against your private AML instance. As mentioned in the instructions, make sure to take note of the output of the command as you will need the clientId , clientSecret , and tenantId values to configure the .env.sh file to run the AML tests. Create a new Azure Machine Learning Workspace . You will need the --resource-group and --workspace-name values you specified during workspace creation to configure the .env.sh file to run the AML tests. You will also need the subscription id that you created the AML workspace in. You can check this by running: az account show --query id Create an AML Compute cluster or AML Compute Instance . You will need the --name that you specified during compute cluster/instance creation to configure the .env.sh file to run the AML tests.","title":"Dev Build"},{"location":"getting-started/dev-build/#prerequisites","text":"Python 3.8 . Note that 3.9 is not currently supported due to Azure Machine Learning dependencies. Poetry 1.1.7 or higher. Azure CLI 2.27 or higher. Azure Functions Work Tools v3.x or higher (Optional). Clone the repo to your local machine and initialize the submodules: git clone https://github.com/SAME-Project/same-mono-private.git cd same-mono-private git submodule update --init --recursive Download and install Poetry, which is used to manage dependencies and virtual environments for the SAME project. You will need to install the project's Python dependencies using Poetry as well after installing it: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python3 - poetry install To install AML dependencies, now optional, use poetry install --extras azureml","title":"Prerequisites"},{"location":"getting-started/dev-build/#using-the-devcontainer-with-visual-studio-code","text":"If you are using Visual Studio Code (VSCode) to work with the same-mono-private repo, VSCode supports development in a containerized environment through its Remote - Container extension , so you don't need to manually install all of the tools and frameworks yourself.","title":"Using the devcontainer with Visual Studio Code"},{"location":"getting-started/dev-build/#prerequisites_1","text":"Docker For Windows users, we recommend enabling WSL2 back-end integration with Docker . Visual Studio Code Visual Studio Code Remote - Containers extension","title":"Prerequisites"},{"location":"getting-started/dev-build/#opening-same-mono-private-in-a-devcontainer","text":"After you have cloned the dapr repo locally, open the dapr folder in VSCode. For example: git clone https://github.com/SAME-Project/same-mono-private.git cd same-mono-private git submodule update --init --recursive code . VSCode will detect the presence of a dev container definition in the repo and will prompt you to reopen the project in a container. Alternatively, you can open the command palette and use the Remote-Containers: Reopen in Container command. Once the container is loaded, open an integrated terminal in VSCode and you're ready to use the repo. You will still need to install the project Python dependencies using the preinstalled Poetry tool: poetry install To install AML dependencies, now optional, use poetry install --extras azureml","title":"Opening same-mono-private in a devcontainer"},{"location":"getting-started/dev-build/#using-the-repo","text":"Use of the SAME python project assumes executing in a virtual environment managed by Poetry. Before running any commands, the virtual environment should be started: poetry shell NOTE: From this point forward, all functions require executing inside that virtual environment. If you see an error like zsh: command not found , it could be because you're not executing inside the venv. You can check this by executing: which python3 This should result in a response like: .../pypoetry/virtualenvs/same-mono-private-88mixeKa-py3.8/bin/python3 . If it reports something like /usr/bin/python or /usr/local/bin/python , you are using the system python, and things will not work as expected.","title":"Using the repo"},{"location":"getting-started/dev-build/#how-to-execute-against-a-notebook-from-source-code","text":"From the root of project, execute: python3 cli/same/main.py <cli-arguments> TODO: Enable building the CLI into a redistributable binary via something like PyOxidiser in same-mono-private. When we get to binary builds of the CLI that can be run locally, you can execute the local build with: bin/same <cli-arguments> After we start publishing builds, you can install and execute with the following: curl -L0 https://get.sameproject.org/ | bash - same <cli-arguments>","title":"How to execute against a notebook from source code"},{"location":"getting-started/dev-build/#how-to-run-the-tests-in-the-repo","text":"Setup kubeconfig to Azure Kubernetes Service configured for Kubeflow To run the CLI pytests, you will need a Kubernetes cluster configured with Kubeflow. We already have a Kubernetes cluster set up in the SAME-sample-vm_group resource group. To use that cluster as a member of the SAME Dev subscription, import the cluster credentials to your local kubeconfig as the current context: az login az account set -s \"SAME Dev\" az aks get-credentials --name AKSMLProductionCluster --resource-group SAME-sample-vm_group NOTE: You will also need to be logged into Azure with the SAME Dev subscription every time to run the tests. The Azure login will also allow the Durable Functions backend tests to run. Setup the environment variables for Azure Machine Learning (AML) To run the AML tests, the local environment variables for AML must be populated. You can set them by providing a file named .env.sh at the root of the directory containing the following export statements: # Set using the following instructions: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication#configure-a-service-principal export AML_SP_APP_ID = <clientId from ` az ad sp create-for-rbac ` > export AML_SP_TENANT_ID = <tenantId from ` az ad sp create-for-rbac ` > export AML_SP_PASSWORD_VALUE = <clientSecret from ` az ad sp create-for-rbac ` > # Set using the following instructions: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli#set-up export WORKSPACE_SUBSCRIPTION_ID = <id from ` az account show ` when creating the workspace> export WORKSPACE_RESOURCE_GROUP = <resource-group value passed to ` az ml workspace create ` > export WORKSPACE_NAME = <workgroup-name value passed to ` az ml workspace create ` > # Compute name instance that can be setup using: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-manage-compute-instance?tabs=azure-cli#create export AML_COMPUTE_NAME = <name passed to ` az ml compute create ` > For example, using the pre-configured SAME-test-aml-rg available to the SAME Dev subscription: #!/usr/bin/env bash export AML_SP_APP_ID = \"2dd71611-83d6-4950-b4b2-ccfb6efe6528\" export AML_SP_TENANT_ID = \"72f988bf-86f1-41af-91ab-2d7cd011db47\" export AML_SP_PASSWORD_VALUE = <copy from same-aml-test-sp ` clientSecret ` in the same-infra-keyvault> export WORKSPACE_SUBSCRIPTION_ID = \"1367ca4d-9e6c-4c41-937f-c657878ee8d5\" export WORKSPACE_RESOURCE_GROUP = \"SAME-test-aml-rg\" export WORKSPACE_NAME = \"SAME-test-aml-workspace\" export AML_COMPUTE_NAME = \"SAME-test-aml-compute\" Run the tests To run all the tests against the CLI and SDK: pytest To run a subset of tests for a single file: pytest test/cli/test_<file>.py -k \"test_<name>\"","title":"How to run the tests in the repo"},{"location":"getting-started/dev-build/#how-to-setup-private-test-environments","text":"","title":"How to setup private test environments"},{"location":"getting-started/dev-build/#local-kubeflow-cluster-on-minikube-in-devcontainer","text":"The devcontainer image for same-mono-private comes with minikube preinstalled, so you can set up a local Kubeflow cluster to run the CLI pytests against if you wish: Start a minikube cluster in the devcontainer: Note: Kubeflow currently defines its Custom Resource Definitions (CRD) under apiextensions.k8s.io/v1beta which is deprecated in Kubernetes v1.22, so minikube must start the cluster with a version <1.22. See kubeflow/kfctl issue #500 . minikube start --kubernetes-version = v1.21.5 Starting minikube will also change the default kubeconfig context to the minikube cluster. You can check this with: kubectl config get-contexts Deploy Kubeflow to the minikube cluster: export PIPELINE_VERSION = 1 .7.0 kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= $PIPELINE_VERSION \" kubectl wait --for condition = established --timeout = 60s crd/applications.app.k8s.io kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= $PIPELINE_VERSION \"","title":"Local Kubeflow cluster on Minikube in devcontainer"},{"location":"getting-started/dev-build/#kubeflow-cluster-on-azure-kubernetes-services-aks","text":"From any Azure subscription where you are at least a Contributor, you can create and provision a new AKS cluster with Kubeflow: Create a new AKS cluster either using the Azure CLI or Azure Portal . The linked instructions will also update your kubeconfig to use the new cluster as the context when you run az aks get-credentials , but you can also manually do so with: kubectl config set-context <context name> Deploy Kubeflow to the cluster. Note: The document references a non-existent v1.3.0 release, you can simply use the v1.2.0 release instead. See kubeflow/kfctl issue #495 .","title":"Kubeflow cluster on Azure Kubernetes Services (AKS)"},{"location":"getting-started/dev-build/#azure-machine-learning-aml-workspace-and-compute","text":"Create a new Service Principal for running tests against your private AML instance. As mentioned in the instructions, make sure to take note of the output of the command as you will need the clientId , clientSecret , and tenantId values to configure the .env.sh file to run the AML tests. Create a new Azure Machine Learning Workspace . You will need the --resource-group and --workspace-name values you specified during workspace creation to configure the .env.sh file to run the AML tests. You will also need the subscription id that you created the AML workspace in. You can check this by running: az account show --query id Create an AML Compute cluster or AML Compute Instance . You will need the --name that you specified during compute cluster/instance creation to configure the .env.sh file to run the AML tests.","title":"Azure Machine Learning (AML) workspace and compute"},{"location":"getting-started/first-notebook/","text":"Clone the Sample Repo \u00b6 To get started, first you'll need to clone our sample repo. To do so, execute the following command: git clone git@github.com:SAME-Project/SAME-samples.git Next, change into the hello-world directory: cd hello-world poetry shell Note If you do not have poetry installed, this will fail. Make sure you do python -m pip install poetry if this is not present. Finally, deploy your notebook to your environment. If you are using kubeflow, you would execute the following command: same program run -t kubeflow This command converts the notebook into a single python script, and deploys it to your previously configured Kubeflow. To do the same against Azure Machine Learning, you should execute the following command: same program run -t aml","title":"First Notebook"},{"location":"getting-started/first-notebook/#clone-the-sample-repo","text":"To get started, first you'll need to clone our sample repo. To do so, execute the following command: git clone git@github.com:SAME-Project/SAME-samples.git Next, change into the hello-world directory: cd hello-world poetry shell Note If you do not have poetry installed, this will fail. Make sure you do python -m pip install poetry if this is not present. Finally, deploy your notebook to your environment. If you are using kubeflow, you would execute the following command: same program run -t kubeflow This command converts the notebook into a single python script, and deploys it to your previously configured Kubeflow. To do the same against Azure Machine Learning, you should execute the following command: same program run -t aml","title":"Clone the Sample Repo"},{"location":"getting-started/importing-packages/","text":"Background around Python Packages in Notebooks \u00b6 The SAME SDK helps data scientists avoid one of the most common problems in the development, forgetting to include their dependencies as part of their deployments. For example, it is very common to do something like the following when running a notebook: import foo import bar.qaz from qux import moo ! pip install baz All of the above work well when run locally, but if a data scientist is not diligent, and update their requirements.txt file (and commit that update to the code base), when run in prodcution, the code will not work (normally showing an error like \"module not found\"). Apart from just being annoying, this could also take a long time to detect, due to the length of time before pipeline jobs are executed. To aleviate this, the same-sdk enables both local import (using caching and cleaner outputs) and keeps the environment file in sync. Importing Packages via SAME \u00b6 To use same in a notebook, first one has to install it: pip install same-sdk Then, inside the notebook, instead of doing import same , a data scientist would execute: same . import ( \"package_name\" ) This will load the module into python's sys.modules . Later, when same program run is executed, it updates the environment.yaml file to the latest for all system modules for the entire notebook, and uses that file as the record for injecting requirements to the back end system.","title":"Importing Packages"},{"location":"getting-started/importing-packages/#background-around-python-packages-in-notebooks","text":"The SAME SDK helps data scientists avoid one of the most common problems in the development, forgetting to include their dependencies as part of their deployments. For example, it is very common to do something like the following when running a notebook: import foo import bar.qaz from qux import moo ! pip install baz All of the above work well when run locally, but if a data scientist is not diligent, and update their requirements.txt file (and commit that update to the code base), when run in prodcution, the code will not work (normally showing an error like \"module not found\"). Apart from just being annoying, this could also take a long time to detect, due to the length of time before pipeline jobs are executed. To aleviate this, the same-sdk enables both local import (using caching and cleaner outputs) and keeps the environment file in sync.","title":"Background around Python Packages in Notebooks"},{"location":"getting-started/importing-packages/#importing-packages-via-same","text":"To use same in a notebook, first one has to install it: pip install same-sdk Then, inside the notebook, instead of doing import same , a data scientist would execute: same . import ( \"package_name\" ) This will load the module into python's sys.modules . Later, when same program run is executed, it updates the environment.yaml file to the latest for all system modules for the entire notebook, and uses that file as the record for injecting requirements to the back end system.","title":"Importing Packages via SAME"},{"location":"getting-started/installing/","text":"\u26a0 Warning \u00b6 The instructions for installing SAME CLI binary tool will use the version generated from the archived https://github.com/azure-octo/same-cli private repo. To use the updated same-mono-private version of the tool, you will need to run it from source code as described in How to set up your environment for running tests and building SAME . Installing SAME \u00b6 In order to use the SAME CLI to build and deploy your code, you first need to install the SAME binary. This is a no-dependency binary that runs on various operating systems, with pre-built binaries for recent versions of Windows, MacOS and Linux. {{< tabs From-Source Linux MacOS Windows Binaries >}} {{% codetab %}} It is currently recommended to clone the repo and run SAME CLI directly from the Python virtual environment. For details, refer to How to set up your environment for running tests and building SAME . {{% /codetab %}} {{% codetab %}} Execute the following command to install SAME on Linux. curl -L0 https://get.sameproject.org/ | bash - {{% /codetab %}} {{% codetab %}} Execute the following command to install SAME on MacOS. curl -L0 https://get.sameproject.org/ | bash - {{% /codetab %}} {{% codetab %}} # Not supported {{% /codetab %}} {{% codetab %}} Download the same CLI from the pre-built packages and signatures: https://github.com/SAME-Project/SAMPLE-CLI-TESTER/releases Ensure the user has permission to execute the binary and place it somewhere on your PATH so it can be invoked easily. {{% /codetab %}} {{< /tabs >}} Connecting to a Workflow engine \u00b6 To run SAME, you will need a workflow engine to connect to. We support a variety of workflow engines, but recommend that, for now, you connect to one that is dedicated to SAME exclusively. Below are installation and configuration instructions for each. {{< tabs Kubernetes-Kubeflow Azure-Machine-Learning Sagemaker Airflow >}} {{% codetab %}} Verify that you have a Kubernetes cluster configured with a local kubectl context set as the default. To verify this, run kubectl config current-context and verify that it returns the name of your cluster. Install Kubeflow on Kubernetes. We recommend using a Terrachain from Combinator.ml. https://combinator.ml/stacks/kubeflow-mlflow/ / Note: We support Kubernetes in Docker as well, though this can be quite resource intensive. You may wait up to 10 minutes for the service to become available after the first installation. Ensure your $KUBECONFIG is set. SAME can operate without this, but some tools may expect this to be set. set KUBECONFIG = \"~/.kube/config\" {{% /codetab %}} {{% codetab %}} In order to use Azure Machine Learning as a target, you will need to: Authorize with Azure Set a series of environment variables that describe your workspace Provision the necessary compute ahead of time. We will walk through all of these now. Authorizing with Azure \u00b6 First, you should download and install the Azure CLI - https://docs.microsoft.com/en-us/cli/azure/install-azure-cli You will also need to create a service principal for accessing the AML workspace. You can follow the instructions for doing that here. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace-cli?tabs=createnewresources%2Cvnetpleconfigurationsv1cli Setting Your Environment Variables \u00b6 After executing this, you will get a JSON blob output that you will need to set as environment variables. You will need to set all of the following: export AML_SP_NAME = \"xxxx\" export AML_SP_APP_ID = \"<UID>\" export AML_SP_OBJECT_ID = \"<UID>\" export AML_SP_TENANT_ID = \"<UID>\" export AML_SP_PASSWORD_VALUE = \"**PASSWORD**\" export AML_SP_PASSWORD_ID = \"<UID>\" export WORKSPACE_SUBSCRIPTION_ID = \"<UID>\" export WORKSPACE_RESOURCE_GROUP = \"Resource_Group_Name\" export WORKSPACE_NAME = \"WORKSPACE_NAME\" Setting Up Compute cluster \u00b6 Finally, you will need to setup your compute cluster(s) to target with SAME. The instructions to do that are here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=azure-cli {{% /codetab %}} {{% codetab %}} -- Not Implemented -- {{% /codetab %}} {{% codetab %}} -- Not Implemented -- {{% /codetab %}} {{< /tabs >}} Next \u00b6 You're done setting up SAME and are now ready to execute! The default execution uses Kubeflow (either locally or in the cloud). Use the -t flag to set another target.","title":"Installing"},{"location":"getting-started/installing/#warning","text":"The instructions for installing SAME CLI binary tool will use the version generated from the archived https://github.com/azure-octo/same-cli private repo. To use the updated same-mono-private version of the tool, you will need to run it from source code as described in How to set up your environment for running tests and building SAME .","title":"\u26a0 Warning"},{"location":"getting-started/installing/#installing-same","text":"In order to use the SAME CLI to build and deploy your code, you first need to install the SAME binary. This is a no-dependency binary that runs on various operating systems, with pre-built binaries for recent versions of Windows, MacOS and Linux. {{< tabs From-Source Linux MacOS Windows Binaries >}} {{% codetab %}} It is currently recommended to clone the repo and run SAME CLI directly from the Python virtual environment. For details, refer to How to set up your environment for running tests and building SAME . {{% /codetab %}} {{% codetab %}} Execute the following command to install SAME on Linux. curl -L0 https://get.sameproject.org/ | bash - {{% /codetab %}} {{% codetab %}} Execute the following command to install SAME on MacOS. curl -L0 https://get.sameproject.org/ | bash - {{% /codetab %}} {{% codetab %}} # Not supported {{% /codetab %}} {{% codetab %}} Download the same CLI from the pre-built packages and signatures: https://github.com/SAME-Project/SAMPLE-CLI-TESTER/releases Ensure the user has permission to execute the binary and place it somewhere on your PATH so it can be invoked easily. {{% /codetab %}} {{< /tabs >}}","title":"Installing SAME"},{"location":"getting-started/installing/#connecting-to-a-workflow-engine","text":"To run SAME, you will need a workflow engine to connect to. We support a variety of workflow engines, but recommend that, for now, you connect to one that is dedicated to SAME exclusively. Below are installation and configuration instructions for each. {{< tabs Kubernetes-Kubeflow Azure-Machine-Learning Sagemaker Airflow >}} {{% codetab %}} Verify that you have a Kubernetes cluster configured with a local kubectl context set as the default. To verify this, run kubectl config current-context and verify that it returns the name of your cluster. Install Kubeflow on Kubernetes. We recommend using a Terrachain from Combinator.ml. https://combinator.ml/stacks/kubeflow-mlflow/ / Note: We support Kubernetes in Docker as well, though this can be quite resource intensive. You may wait up to 10 minutes for the service to become available after the first installation. Ensure your $KUBECONFIG is set. SAME can operate without this, but some tools may expect this to be set. set KUBECONFIG = \"~/.kube/config\" {{% /codetab %}} {{% codetab %}} In order to use Azure Machine Learning as a target, you will need to: Authorize with Azure Set a series of environment variables that describe your workspace Provision the necessary compute ahead of time. We will walk through all of these now.","title":"Connecting to a Workflow engine"},{"location":"getting-started/installing/#authorizing-with-azure","text":"First, you should download and install the Azure CLI - https://docs.microsoft.com/en-us/cli/azure/install-azure-cli You will also need to create a service principal for accessing the AML workspace. You can follow the instructions for doing that here. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace-cli?tabs=createnewresources%2Cvnetpleconfigurationsv1cli","title":"Authorizing with Azure"},{"location":"getting-started/installing/#setting-your-environment-variables","text":"After executing this, you will get a JSON blob output that you will need to set as environment variables. You will need to set all of the following: export AML_SP_NAME = \"xxxx\" export AML_SP_APP_ID = \"<UID>\" export AML_SP_OBJECT_ID = \"<UID>\" export AML_SP_TENANT_ID = \"<UID>\" export AML_SP_PASSWORD_VALUE = \"**PASSWORD**\" export AML_SP_PASSWORD_ID = \"<UID>\" export WORKSPACE_SUBSCRIPTION_ID = \"<UID>\" export WORKSPACE_RESOURCE_GROUP = \"Resource_Group_Name\" export WORKSPACE_NAME = \"WORKSPACE_NAME\"","title":"Setting Your Environment Variables"},{"location":"getting-started/installing/#setting-up-compute-cluster","text":"Finally, you will need to setup your compute cluster(s) to target with SAME. The instructions to do that are here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=azure-cli {{% /codetab %}} {{% codetab %}} -- Not Implemented -- {{% /codetab %}} {{% codetab %}} -- Not Implemented -- {{% /codetab %}} {{< /tabs >}}","title":"Setting Up Compute cluster"},{"location":"getting-started/installing/#next","text":"You're done setting up SAME and are now ready to execute! The default execution uses Kubeflow (either locally or in the cloud). Use the -t flag to set another target.","title":"Next"},{"location":"getting-started/sample-notebook/","text":".notebook-links {display: flex; margin: 1em 0;} .notebook-links a {padding: .75em; margin-right: .75em; font-weight: bold;} a.colab-link { padding-left: 3.25em; background-image: url(/docs/images/logos/colab.ico); background-repeat: no-repeat; background-size: contain; } a.github-link { padding-left: 2.75em; background-image: url(/docs/images/logos/github.png); background-repeat: no-repeat; background-size: auto 75%; background-position: left center; } Run in Google Colab View source on GitHub dataset = 'sample_data' gpu_type = 'A100' import tensorflow import datetime print ( f \"Time: { datetime . datetime . now () } \" ) a = 10 b = a + 5 #15 from IPython.display import Image url = 'https://raw.githubusercontent.com/SAME-Project/SAME-samples/main/test-artifacts/FaroeIslands.jpeg' print ( f \"Time: { datetime . datetime . now () } \" ) a = a + 5 b = b + 10 #25 from IPython import display display . Image ( url ) import plotly print ( f \"Time: { datetime . datetime . now () } \" ) def some_math ( x , z ) -> tuple : return ( round ( x + z , 2 ), round ( x / z , 2 )) a = a * 20 b = b * 100 #2500 print ( f \"B = { b } \" ) import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats print ( f \"Time: { datetime . datetime . now () } \" ) mu = 0 std = 1 x = np . linspace ( start =- 4 , stop = 4 , num = 100 ) y = stats . norm . pdf ( x , mu , std ) a = a + 5 b = b + 10 # 2515 plt . plot ( x , y ) plt . show () import requests import pandas as pd import plotly.figure_factory as ff import chart_studio.plotly as py print ( f \"Time: { datetime . datetime . now () } \" ) url = 'https://raw.githubusercontent.com/SAME-Project/SAME-samples/main/test-artifacts/test.csv' df = pd . read_csv ( url ) a = a * 1000 b = b / 67 # 37.5373134328 df . describe () a = a + 5 b = b + 10 # 47.5373134328 print ( f \"Time: { datetime . datetime . now () } \" ) g = some_math ( 8 , 21 ) print ( f \"Time: { datetime . datetime . now () } \" ) j = g [ 0 ] k = g [ 1 ] print ( f \"Time: { datetime . datetime . now () } \" ) a = a + 5 b = b + 10 # 57.5373134328 print ( f \"j: { j } \" ) print ( f \"k: { k } \" ) print ( f \"Time: { datetime . datetime . now () } \" ) a = a + 5 b = b + 10 # 67.5373134328 print ( \"0.0.2\" ) print ( f \"Time: { datetime . datetime . now () } \" ) print ( f \"Accessing the value of B: { b } \" ) print ( f \"Time: { datetime . datetime . now () } \" ) Run in Google Colab View source on GitHub","title":"Sample Notebook"}]}